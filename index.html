<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>CS 159: Advanced Topics in Machine Learning: Structured Prediction</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<link href="default.css" rel="stylesheet" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="http://identity.caltech.edu/site_images/275-caltech-icon-orange-rgb.png" />

</head>
<body>

   <h1>CS 159: Advanced Topics in Machine Learning: Structured Prediction </h1>

   <br>
   2016/2017 Spring Term (<a href="http://www.yisongyue.com/courses/cs159/">previous year</a>)



<a name="desc"></a>
<h3>Course Description</h3>
<p class="indented">
This course will cover a mixture of the following topics:
<ul>
   <li> Graphical Models
   <li> Inference Methods
   <ul> 
        <li> Message Passing, Integer Programs, Dynamic Programming, Variational Methods
   </ul>
   <li> Classical Discriminative Learning
   <ul> 
        <li> Structured SVM, Structured Perceptron, Conditional Random Fields
   </ul>
   <li> Non-Linear Approaches
   <ul> 
        <li> Structured Random Forests, Deep Structured Prediction
   </ul>
   <li> More Complex Structures
   <ul>
        <li> Hierarchical Classification, Sequence Prediction/Generation
   </ul>
   <li> Applications to Computer Vision, Speech Recognition, Natural Language Processing, etc
</ul>
</p>

<h3>Course Details</h3>
<p class="indented">
<ul>
<li>Lectures on Tu/Th at 1pm-2:30pm in Annenberg 105</li>
<li>This is a paper reading course, where we read and discuss research papers in class</li>
<li>Student participation is required, including presenting papers in class (20% of total grade)</li>
<li>Mini-quiz on papers for every lecture, given after lecture/discussion (10% of total grade)</li>
<li>Final project that explores some topic covered in class (70% of final grade)</li>
<li>Piazza Forum: <a href="https://piazza.com/caltech/spring2017/cs159">link</a></li> 
</ul>



<h3>Instructors</h3>
<p class="indented">
<a href="http://ttic.uchicago.edu/~taehwan">Taehwan Kim</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;taehwan@caltech.edu<br>
<a href="http://www.yisongyue.com">Yisong Yue</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yyue@caltech.edu<br>
</p>

<h3>Teaching Assistants</h3>
<p class="indented">
<a href="http://hoangminhle.github.io/">Hoang Le</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hmle@caltech.edu<br>
Jialin Song</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jssong@caltech.edu<br>
<a href="http://stephanzheng.github.io/">Stephan Zheng</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stephan@caltech.edu<br>
</p>


<h3>Office Hours</h3>
<iframe src="https://calendar.google.com/calendar/embed?showTitle=0&amp;mode=WEEK&amp;height=600&amp;wkst=1&amp;bgcolor=%23FFFFFF&amp;src=dvolu8hvpm63i4kv98ga00md6s%40group.calendar.google.com&amp;color=%23B1365F&amp;ctz=America%2FLos_Angeles" style="border-width:0" width="800" height="600" frameborder="0" scrolling="no"></iframe>

<a name="data"></a>
<h3> Datasets and Codes</h3>

<p class="indented"> 
<br>Could be useful for final project.<br><br>
<p class='indented'>
<ul>
     <li> <a href="http://make3d.cs.cornell.edu/">Make3D: Convert your still image into 3D model</a></li>
     <li> <a href="http://melodi.ee.washington.edu/gmtk/">The Graphical Models Toolkit</a></li>
     <li> <a href="https://keras.io/">Keras: Deep Learning library for Theano and TensorFlow</a></li>
</ul>
</p>

<a name="lectures"></a>
<h3> Presentation Schedule</h3>

<p class="indented"> 
<br>Note: schedule is subject to change.<br><br>

<table class="indented">
   <tr>
      <td> <b>Date</b></td>
      <td> <b>Papers</b> </td>
      <td> <b>Presenters</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td></td>
      <td> <b>Materials</b</td>
   </tr>
   <tr>
      <td> 4/4/2017&nbsp;</td>
      <td> Introduction &amp; Administrivia&nbsp;<br> Prababilistic Graphical Models&nbsp;<br> Inference Methods</td>
      <td> Taehwan Kim </td>
      <td>[<a href="lectures/lecture_04_04.pdf">slides</a>]</td>
      <td> <li><a href="http://www.cis.upenn.edu/~mkearns/papers/barbados/jordan-tut.pdf">An introduction to graphical models</a> <br>
         <li><a href="http://mlg.eng.cam.ac.uk/zoubin/course04/hbtnn2e-I.pdf">Probabilistic inference in graphical models</a> 
      </td>
   </tr>
   <tr>
      <td> 4/6/2017&nbsp;</td>
      <td> Learning for Structured Prediction<br> Structured Perceptrons &amp; Structural SVMs</td>
      <td> Yisong Yue </td>
      <td>[<a href="lectures/lecture_04_06.pdf">slides</a>]</td>
      <td> <li><a href="http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d4/data/pdf/anthology-PDF/W/W02/W02-1001.pdf">Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</a> <br>
         <li><a href="http://www.jmlr.org/papers/volume6/tsochantaridis05a/tsochantaridis05a.pdf">Large Margin Methods for Structured and Interdependent Output Variables</a>
      </td>
   </tr>
   <tr>
      <td> 4/11/2017</td>
      <td> Conditional Random Fields</td>
      <td> Milan Cvitkovic,<br> Grant Van Horn<br><b>Mentor:</b> Taehwan Kim</td>
      <!--<td>[<a href="lectures/oco.pdf">slides</a>]</td>-->
      <td>[<a href="lectures/lecture_04_11.pdf">slides</a>]</td>
      <td> <li><a href="https://faculty.cs.byu.edu/~ringger/CS479/papers/LaffertyMcCallumPereira-CRF-icml01.pdf">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</a> <br>
          <li><a href="https://papers.nips.cc/paper/2652-conditional-random-fields-for-object-recognition.pdf">Conditional Random Fields for Object Recognition</a><br>
          <li> <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf">Conditional Random Fields as Recurrent Neural Networks</a><br>
          <li> <a href="http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf"> Introduction to Conditional Random Field </a> (optional: a comprehensive tutorial)
      </td>
   </tr>
   <tr>
      <td> 4/13/2017</td>
      <td> Message Passing &amp;<br> Linear Programing</td>
      <td> Hoang Le,<br> Jialin Song<br></td>
      <td>[<a href="lectures/lecture_04_13_1.pdf">slides1</a>][<a href="lectures/lecture_04_13_2.pdf">slides2</a>]</td>
      <td> <li><a href="http://mlg.eng.cam.ac.uk/zoubin/course04/hbtnn2e-I.pdf">Probabilistic inference in graphical models</a><br>
          <li> Chapter 8.2 and 8.4 in <a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference</a><br>
          <li> <a href="http://papers.nips.cc/paper/3200-fixing-max-product-convergent-message-passing-algorithms-for-map-lp-relaxations.pdf"> Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations </a>
      </td>
   </tr>
   <tr>
      <td> 4/18/2017</td>
      <td> Variational Inference</td>
      <td> Kun Ho (John) Kim,<br> Albert Zhao<br><b>Mentor:</b> Taehwan Kim</td>
      <td>[<a href="lectures/lecture_04_18.pdf">slides</a>]</td>
      <td> <li><a href="https://arxiv.org/pdf/1601.00670.pdf">Variational Inference: A Review for Statisticians</a> <br>
          <li> <a href="https://las.inf.ethz.ch/files/gomes11crowd.pdf">Crowdclustering</a><br>
          <li> <a href="https://arxiv.org/pdf/1606.05908.pdf"> Tutorial on Variational Autoencoders </a> (optional)
      </td>
   </tr>
   <tr>
      <td> 4/20/2017</td>
      <td> Sampling Methods</td>
      <td> Sara Beery, <br> Natalie Bernat, <br> Eric Zhan<br><b>Mentor:</b> Hoang Le</td>
      <td>[<a href="lectures/lecture_04_20.pdf">slides</a>]</td>
      <td> <li> <a href="http://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf">An Introduction to MCMC for Machine Learning</a> <br>
          <li><a href="https://smartech.gatech.edu/bitstream/handle/1853/3246/03-35.pdf?sequence=1&isAllowed=y">An MCMC-based Particle Filter for Tracking Multiple Interacting Targets</a>
      </td>
   </tr>
   <tr>
      <td> 4/25/2017</td>
      <td> Hidden Markov Models (HMM)</td>
      <td> Gabriela Tavares, <br> Juri Minxha <br> <b>Mentor:</b> Taehwan Kim</td>
      <td>[slides]</td>
      <!--<td>[<a href="lectures/lecture_04_11.pdf">slides</a>]</td>-->
      <td> <li> <a href="https://papers.nips.cc/paper/3051-large-margin-hidden-markov-models-for-automatic-speech-recognition.pdf">Large margin hidden Markov models for automatic speech recognition</a> <br>
          <li><a href="http://www.aaai.org/Papers/ICML/2003/ICML03-004.pdf">Hidden Markov Support Vector Machines</a><br>
          <li><a href="https://www.cs.toronto.edu/~gdahl/papers/DBN4LVCSR-TransASLP.pdf">Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition</a>  
      </td>
   </tr>
   <tr>
      <td> 4/27/2017</td>
      <td> Topic Model</td>
      <td> Zach Wu, <br> Kevin Yang<br><b>Mentor:</b> Taehwan Kim</td>
      <td>[slides]</td>
      <!--<td>[<a href="lectures/lecture_04_11.pdf">slides</a>]</td>-->
      <td> <li><a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Latent Dirichlet Allocation</a><br>
          <li><a href="http://menome.com/wp/wp-content/uploads/2014/12/Blei2011.pdf">Introduction to Probabilistic Topic Models</a><br>
          <li><a href="https://www.cics.umass.edu/~mccallum/papers/tot-kdd06s.pdf">Topics over Time: A Non-Markov Continuous-Time Model of Topical Trends</a>
      </td>
   </tr>
   <!--
   <tr>
      <td> 5/2/2017</td>
      <td> Hierarchical / Extreme Classification</td>
      <td> Anish Thilagar, <br>  Ashwin Balakrishna <br><b>Mentor:</b> Jialin Song</td>
      <td>[slides]</td>
      <!--<td>[<a href="lectures/lecture_04_11.pdf">slides</a>]</td>
      <td> <li> <a href="http://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf">An Introduction to MCMC for Machine Learning</a> <br>
          <li><a href="https://smartech.gatech.edu/bitstream/handle/1853/3246/03-35.pdf?sequence=1&isAllowed=y">An MCMC-based Particle Filter for Tracking Multiple Interacting Targets</a>
      </td>
   </tr>

 -->
</table>

<a name="papers"></a>
<h3>Topic and Reading List</h3>
<p class='indented'>
<b><a href="https://docs.google.com/spreadsheets/d/1xRLcw4OnVwZEqAILIZh7TE9nd--ecWnhT9xRx2yCXd0/edit?usp=sharing">Presentation Signup Sheet</a></b>
</p>
<br>  
<p class='indented'>
<ul>
   <li> Inference Methods
   <ul>
        <li> Graph Cuts</li>
        <ul> 
             <li> Boykov et al, <a href="http://www.cs.sfu.ca/CourseCentral/821/li/material/source/Boykov01.pdf">Fast Approximate Energy Minimization via Graph Cuts</a>, TPAMI 2001.  </li>
             <li> Boykov et al, <a href="http://www.cs.duke.edu/brd/Teaching/Bio/asmb/current/Papers/BVZ-cvpr98.pdf">Markov Random Fields with Efficient Approximations</a>, CVPR 1998 </li>
        </ul>
        <li> Linear Programing </li>
        <ul>
             <li> Chapter 8.2 and 8.4 in <a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference</a> by Wainwright and Jordan</li>
             <li>Globerson and Jaakkola, <a href="http://papers.nips.cc/paper/3200-fixing-max-product-convergent-message-passing-algorithms-for-map-lp-relaxations.pdf"> Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations</a>, NIPS 2008</li>
        </ul>
        <li> Variational Inference</li>
        <ul>
             <li> Chapter 5 in <a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference</a> by Wainwright and Jordan</li>
             <li> <a href="https://arxiv.org/pdf/1601.00670.pdf">Variational Inference: A Review for Statisticians</a> by Blei</li>
        </ul>
        <li>Sampling Methods</li>
        <ul>
             <li>Andrieu et al, <a href="http://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf">An Introduction to MCMC for Machine Learning</a>, Machine Learning 2003</li>
             <li> Khan et al, <a href="https://papers.nips.cc/paper/2652-conditional-random-fields-for-object-recognition.pdf">An MCMC-based Particle Filter for Tracking Multiple Interacting Targets</a>, ECCV 2004</li>
        </ul>
   </ul>
   <li> Graphical Models</li>
   <ul>
        <li> Hidden Markov Models (HMM)</li>
        <ul>
             <li>Sha and Saul, <a href="https://papers.nips.cc/paper/3051-large-margin-hidden-markov-models-for-automatic-speech-recognition.pdf">Large margin hidden Markov models for automatic speech recognition</a>, NIPS 2007</li>
             <li>Altun et al, <a href="http://www.aaai.org/Papers/ICML/2003/ICML03-004.pdf">Hidden Markov Support Vector Machines</a>, ICML 2003</li>
             <li>Dahl et al, <a href="https://www.cs.toronto.edu/~gdahl/papers/DBN4LVCSR-TransASLP.pdf">Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition</a>, TASLP 2012</li>
        </ul>
        <li>Conditional Random Fields (CRF)</li>
        <ul>
             <li>Lafferty et al, <a href="https://faculty.cs.byu.edu/~ringger/CS479/papers/LaffertyMcCallumPereira-CRF-icml01.pdf">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</a>, ICML 2001</li>
             <li>Quattoni et al, <a href="https://papers.nips.cc/paper/2652-conditional-random-fields-for-object-recognition.pdf">Conditional Random Fields for Object Recognition</a>, NIPS 2004</li>
             <li> Zheng et al, <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf">Conditional Random Fields as Recurrent Neural Networks</a>, ICCV 2015</li>
             <li>  Sutton and McCallumAn, <a href="http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf"> Introduction to Conditional Random Fields</a></li>
        </ul>
   </ul> 
   <li>Topic Model</li>
   <ul>
        <li>Blei et al, <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Latent Dirichlet Allocation</a>, JMLR 2003</li>
        <li><a href="http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf">Topic models</a> by Blei and Lafferty</li>
   </ul>
   <li>Structured Support Vector Machines (SVM) </li>
   <ul>
       <li> Tsochantaridis et al, <a href="http://www.jmlr.org/papers/volume6/tsochantaridis05a/tsochantaridis05a.pdf">Large Margin Methods for Structured and Interdependent Output Variables</a>, JMLR 2005</li>
       <li> Yu and Joachims <a href="http://www.joachims.org/publications/yu_joachims_09a.pdf">Learning structural SVMs with latent variables</a>, ICML 2009</li>
       <li> Yue and Joachims <a href="http://www.yisongyue.com/publications/icml2008_svmdiv.pdf">Predicting Diverse Subsets Using Structural SVMs</a>, ICML 2008</li>
   </ul>
   <li> Hierarchical / Extreme Classification </li>
   <ul> 
       <li> Cai and Hofmann, <a href="https://info.cis.uab.edu/zhang/Spam-mining-papers/Hierarchical.Data.Classification.with.Support.Vector.Machines.pdf">Hierarchical document categorization with support vector machines</a>, CIKM 2004</li>
       <li> Yen et al, <a href="http://www.cs.utexas.edu/~inderjit/public_papers/pdsparse_icml16.pdf">A Primal and Dual Sparse Approach to Extreme Classification</a>, ICML 2016</li>
   </ul>
   <li> Structured Perceptron </li>
   <ul>
       <li> Collins, <a href="http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d4/data/pdf/anthology-PDF/W/W02/W02-1001.pdf">Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</a>, ACL 2002</li>
       <li> McDonald et al, <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36266.pdf">Distributed Training Strategies for the Structured Perceptron</a>, NAACL 2010</li>
   </ul>
   <li> Structured Random Forests </li>
   <ul>
       <li> Dollar and Zitnick, <a href="https://pdollar.github.io/files/papers/DollarICCV13edges.pdf">Structured Forests for Fast Edge Detection</a>, ICCV 2013</li>
       <li> Kontschieder et al, <a href="http://www.dsi.unive.it/~srotabul/files/publications/iccv11.pdf">Structured Class-Labels in Random Forests for Semantic Image Labelling</a>, ICCV 2011</li>
   </ul>
   <li> Deep Structured Models</li>
   <ul>
       <li>Graphical Model + Deep Learning</li>
       <ul>
            <li>Chen et al, <a href="https://arxiv.org/pdf/1407.2538">Learning Deep Structured Models</a>, ICML 2015</li>
            <li>Schwing and Urtasun, <a href="https://arxiv.org/pdf/1503.02351">Fully Connected Deep Structured Networks</a>, arxiv 2015</li>
            <li> Johnson et al, <a href="http://papers.nips.cc/paper/6379-composing-graphical-models-with-neural-networks-for-structured-representations-and-fast-inference.pdf">Composing graphical models with neural networks for structured representations and fast inference</a>, NIPS 2016</li>
            <li>Chen et al, <a href="https://arxiv.org/pdf/1412.7062">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a>, ICLR 2015</li>
            <li>Kim et al, <a href="https://arxiv.org/abs/1702.00887">Structured Attention Networks</a>, arxiv 2017</li>
       </ul>
       <li> Deep (Convolutional) Neural Networks</li>
       <ul>
            <li>Sohn et al, <a href="https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf">Learning Structured Output Representation using Deep Conditional Generative Models</a>, NIPS 2015</li>
            <li>Dosovitskiy et al, <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf">Learning to Generate Chairs with Convolutional Neural Networks</a>, CVPR 2015</li>
            <li>Stewart and Ermon, <a href="https://arxiv.org/pdf/1609.05566">Label-Free Supervision of Neural Networks with Physics and Domain Knowledge</a>, arxiv 2016</li>
       </ul>
       <li> Recurrent Neural Networks</li>
       <ul>
            <li> Ranzatto et al, <a href="https://arxiv.org/pdf/1511.06732">Sequence Level Training with Recurrent Neural Networks</a>, arxiv 2015
            <li>Alvarez-Melis and Jaakkola, <a href="https://people.csail.mit.edu/tommi/papers/AlvJaa_ICLR2017.pdf">TREE-STRUCTURED DECODING WITH DOUBLYRECURRENT NEURAL NETWORKS</a>, ICLR 2017</li>
            <li>Deng et al, <a href="https://arxiv.org/pdf/1511.04196.pdf"> Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition</a>, CVPR 2016</li>
       </ul>
   </ul>
   <li> Sequence-to-sequence Model</li>
   <ul>
       <li> Kim et al, <a href="http://www.yisongyue.com/publications/kdd2015_ssw_dt.pdf"> A Decision Tree Framework for Spatiotemporal Sequence Prediction</a>, KDD 2015</li>
       <li> Sutskever et al, <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sequence to Sequence Learning with Neural Networks</a>, NIPS 2014</li>
   </ul>
   <li> Image Captioning and Generation From Text</li>
   <ul>
       <li> Xu et al, <a href="https://arxiv.org/pdf/1502.03044.pdf">Attend and Tell: Neural Image Caption Generation with Visual Attention</a>, ICML 2015</li>
       <li> Mansimov et al, <a href="https://arxiv.org/pdf/1511.02793.pdf">Generating images from captions with attention</a>, ICLR 2016</li>
   </ul>

   <li> Optimization </li>
   <ul>
       <li> Weiss and Taskar, <a href="http://homes.cs.washington.edu/~taskar/pubs/aistats10cascades.pdf">Structured Prediction Cascades</a>, AISTATS 2010</li>
       <li> Shi et al, <a href="https://cs.stanford.edu/~pliang/papers/sample-aistats2015.pdf">Learning Where to Sample in Structured Prediction</a>, AISTATS 2015</li>
   </ul>

   <li> Active Learning</li>
   <ul>
       <li>Shivaswamy and Joachims, <a href="http://www.cs.cornell.edu/people/tj/publications/shivaswamy_joachims_12a.pdf">Online Structured Prediction via Coactive Learning</a>, ICML 2012</li>
       <li> Luo et al, <a href="http://papers.nips.cc/paper/4953-latent-structured-active-learning.pdf">Latent Structured Active Learning</a>, NIPS 2013</li>
   </ul>
<!--
   <li>

   <ul>
       <li> <a href="
       <li> <a href="
       <li> <a href="
   </ul>
-->
 
</ul>
</p> 
<a name="papers2"></a>
<h3>Extended Reference Material (could be useful for picking final project)</h3>
<p class='indented'>Note: some papers belong to multiple categories.</p>


<br>  
<p class='indented'>
<ul>
   <li> Inference Methods
   <ul>
        <li> Graph Cuts</li>
        <ul> 
             <li> Kolmogorov and Zabih, <a href="http://www.cs.cornell.edu/rdz/papers/graph_cuts_pami.pdf">What Energy Functions can be Minimized via Graph Cuts?</a>, TPAMI 2004  </li>
             <li> Kolmogorov and Rother, <a href="http://luthuli.cs.uiuc.edu/~daf/courses/optimization/mrfpapers/04204169.pdf">Minimizing Nonsubmodular Functions with Graph Cuts—A Review</a>, TPAMI 2007 </li>
        </ul>
        <li> Message Passing </li>
        <ul>
             <li> Chapter 2.5 in <a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference</a> by Wainwright and Jordan</li>
        </ul>
        <li> Sampling Methods</li>
        <ul>
             <li><a href="http://courses.cs.washington.edu/courses/cse515/09sp/slides/sampling.pdf">Sampling-Based Inference</a>, Univ. of Washington lecture note</li>
             <li> <a href="http://vcla.stat.ucla.edu/old/MCMC/MCMC_tutorial.htm">Markov Chain Monte Carlo for Computer Vision</a></li>
             <li> <a href="https://media.nips.cc/Conferences/2015/tutorialslides/nips-2015-monte-carlo-tutorial.pdf">Monte Carlo Inference Methods</a>, NIPS 2015 tutorial</li>
             <li> Andrieu et al, <a href="http://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf">An Introduction to MCMC for Machine Learning</a>, Machine Learning 2003</li>
        </ul>

        <li> Variational Inference</li>
        <ul>
             <li><a href="http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf">A Tutorial on Variational Bayesian Inference</a> by Fox and Roberts</li>
             <li>Blei and Jordan, <a href="http://www.cs.columbia.edu/~blei/papers/BleiJordan2004.pdf">Variational Inference for Dirichlet Process Mixtures</a>, Bayesian Analysis 2006</li>
        </ul>
   </ul>
   <li> Graphical Models</li>
   <ul>
        <li> Markov Random Fields (MRF)</li>
        <ul>
             <li>Taskar et al, <a href="https://papers.nips.cc/paper/2397-max-margin-markov-networks.pdf">Max-margin Markov networks</a>, NIPS 2003</li>
             <li>Djolonga et al, <a href="https://las.inf.ethz.ch/files/djolonga16cooperative.pdf">Cooperative Graphical Models</a>, NIPS 2016</li>
        </ul>
        <li>Conditional Random Fields (CRF)</li>
        <ul>
             <li>Sarawagi and Cohen, <a href="www.cs.cmu.edu/~wcohen/postscript/semiCRF.pdf">Semi-Markov Conditional Random Fields for Information Extraction</a>, NIPS 2004</li>
             <li>Ammar et al, <a href="https://arxiv.org/pdf/1411.1147.pdf">Conditional Random Field Autoencoders for Unsupervised Structured Prediction</a>, NIPS 2014</li>
        </ul>
   </ul> 
   <li>Topic Model</li>
   <ul>
        <li>Wang and McCallum, <a href="https://www.cics.umass.edu/~mccallum/papers/tot-kdd06s.pdf">Topics over Time: A Non-Markov Continuous-Time Model of Topical Trends</a>, KDD 2006</li>
        <li>Doshi-Velez et al, <a href="http://hips.seas.harvard.edu/files/doshivelez-graphsparse-aaai-2015.pdf">Graph-Sparse LDA: A Topic Model with Structured Sparsity</a>, AAAI 2015</li>
   </ul>
<!--
   <li>Structured Support Vector Machines (SVM) </li>
   <ul>
       <li> Tsochantaridis et al, <a href="http://www.jmlr.org/papers/volume6/tsochantaridis05a/tsochantaridis05a.pdf">Large Margin Methods for Structured and Interdependent Output Variables</a>, JMLR 2005</li>
       <li> Yu and Joachims <a href="http://www.joachims.org/publications/yu_joachims_09a.pdf">Learning structural SVMs with latent variables</a>, ICML 2009</li>
       <li> Yue and Joachims <a href="http://www.yisongyue.com/publications/icml2008_svmdiv.pdf">Predicting Diverse Subsets Using Structural SVMs</a>, ICML 2008</li>
   </ul>
-->   
   <li> Hierarchical / Extreme Classification </li>
   <ul> 
       <li> Keerthi et al, <a href="http://ntucsu.csie.ntu.edu.tw/~cjlin/papers/sdm_kdd.pdf">A sequential dual method for large scale multi-class linear SVMs</a>, KDD 2008</li>
       <li> Liang et al, <a href="http://www.cs.utexas.edu/~inderjit/public_papers/pdsparse_icml16.pdf">Learning Programs: A Hierarchical Bayesian Approach</a>, ICML 2010</li>
       <li> Prabhu and Varma, <a href="https://manikvarma.github.io/pubs/prabhu14.pdf"> Fastxml: a fast, accurate and stable tree-classifier for extreme multi-label learning</a>, KDD 2014</li>
   </ul>
   <li> Structured Perceptron </li>
   <ul>
       <li> Huang et al, <a href="http://www.coli.uni-saarland.de/~yzhang/rapt-ws1112/papers/huang_2008.pdf">Forest Reranking: Discriminative Parsing with Non-Local Features</a>, ACL 2008</li>
       <li> Liang et al, <a href="https://people.eecs.berkeley.edu/~klein/papers/discriminative-translation.pdf">An End-to-End Discriminative Approach to Machine Translation</a>, ACL 2006</li>
       <li> Neubig et al, <a href="http://www.phontron.com/paper/neubig12emnlp-reordering.pdf">Inducing a Discriminative Parser for Machine Translation Reordering</a>, EMNLP 2012</li>
       <li> Roark et al , <a href="http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d4/data/pdf/anthology-PDF/P/P04/P04-1007.pdf">Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm</a>, ACL 2004</li>       
       <li> McDonald et al, <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36266.pdf">Distributed Training Strategies for the Structured Perceptron</a>, NAACL 2010
</li>
   </ul>
   <li> Structured Random Forests </li>
   <ul>
       <li> Tang et al, <a href="http://www.iis.ee.ic.ac.uk/dtang/cvpr_14.pdf">Latent Regression Forest: Structured Estimation of 3D Articulated Hand Posture</a>, CVPR 2014</li>
   </ul>
   <li> Deep Structured Models</li>
   <ul>
    <!--
       <li>Graphical Model + Deep Learning</li>
       <ul>
            <li>Chen et al, <a href="https://arxiv.org/pdf/1407.2538">Learning Deep Structured Models</a>, ICML 2015</li>
            <li>Schwing and Urtasun, <a href="https://arxiv.org/pdf/1503.02351">Fully Connected Deep Structured Networks</a>, arxiv 2015</li>
            <li> Johnson et al, <a href="http://papers.nips.cc/paper/6379-composing-graphical-models-with-neural-networks-for-structured-representations-and-fast-inference.pdf">Composing graphical models with neural networks for structured representations and fast inference</a>, NIPS 2016</li>
            <li>Chen et al, <a href="https://arxiv.org/pdf/1412.7062">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a>, ICLR 2015</li>
       </ul>
     -->
       <li> Deep (Convolutional) Neural Networks</li>
       <ul>
            <li>Dong et al, <a href="http://papers.nips.cc/paper/5638-analysis-of-variational-bayesian-latent-dirichlet-allocation-weaker-sparsity-than-map.pdf">Learning a Deep Convolutional Network for Image Super‐Resolution</a>, ECCV 2014</li>
            <li>Ren et al, <a href="http://papers.nips.cc/paper/5638-analysis-of-variational-bayesian-latent-dirichlet-allocation-weaker-sparsity-than-map.pdf">Faster r-cnn: Towards real-time object detection with region proposal networks</a>, NIPS 2015</li>
       </ul>
       <!--
       <li> Recurrent Neural Networks</li>
       <ul>
            <li> Ranzatto et al, <a href="https://arxiv.org/pdf/1511.06732">Sequence Level Training with Recurrent Neural Networks</a>, arxiv 2015
            <li>Alvarez-Melis and Jaakkola, <a href="https://people.csail.mit.edu/tommi/papers/AlvJaa_ICLR2017.pdf">TREE-STRUCTURED DECODING WITH DOUBLYRECURRENT NEURAL NETWORKS</a>, ICLR 2017</li>
       </ul>
     -->
   </ul>
   <li> Machine Translation</li>
   <ul>
       <li> Auli et al, <a href="https://michaelauli.github.io/papers/rnn-joint-lm-tm.pdf"> Joint Language and Translation Modeling with Recurrent Neural Networks</a>, EMNLP 2013</li>
       <li> Cho et al, <a href="https://arxiv.org/pdf/1406.1078.pdf">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a>, arxiv 2014</li>
       <li> Bahdanau et al, <a href="https://arxiv.org/pdf/1409.0473">Neural machine translation by jointly learning to align and translate</a>, ICLR 2015</li>
   </ul>
   <li> Image Captioning and Generation From Text</li>
   <ul>
       <li> Kuznetsova et al, <a href="http://www.tamaraberg.com/papers/acl_12.pdf">Collective Generation of Natural Image Descriptions</a>, ACL 2012</li>
       <li> Vinyals et al, <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf">Show and tell: A neural image caption generator</a>, CVPR 2015</li>
       <li> Gregor et al, <a href="https://arxiv.org/pdf/1511.02793.pdf">DRAW: A Recurrent Neural Network For Image Generation</a>, arxiv 2015</li>
   </ul>

   <li> Optimization </li>
   <ul>
       <li> He and Taskar, <a href="http://www.cs.jhu.edu/~jason/papers/he+al.nips14.pdf">Learning to Search in Branch-and-Bound Algorithms</a>, NIPS 2014</li>
       <li> Song et al, <a href="http://www.alexander-schwing.de/papers/SongEtAl_ICML2016.pdf">Training Deep Neural Networks via Direct Loss Minimization</a>, ICML 2016</li>
       <li> Kuleshov and Liang, <a href="https://cs.stanford.edu/~pliang/papers/calibration-nips2015.pdf">Calibrated Structured Prediction</a>, NIPS 2015</li>
       <li> Steinhardt and Liang, <a href="https://arxiv.org/pdf/1502.06668.pdf">Learning Fast-Mixing Models for Structured Prediction</a>, ICML 2015</li>                     
   </ul>

   <li> Others</li>
   <ul>
       <li>Richardson and Domingos, <a href="http://homes.cs.washington.edu/~pedrod/papers/mlj05.pdf">Markov Logic Networks</a>, MLJ 2005</li>
       <li>Krishnan et al, <a href="https://arxiv.org/pdf/1511.05121.pdf">Deep Kalman Filters</a>, arxiv 2015</li>
       <li>Low et al, <a href="http://www.cs.cornell.edu/home/cardie/papers/ozan-nips14drsv.pdf">GraphLab: A New Framework For Parallel Machine Learning</a>, arxiv 2014</li>
       <li>Prasad et al, <a href="https://arxiv.org/abs/1411.1752">Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets</a>, NIPS 2014</li>
       <li>Irsoy et al, <a href="http://www.cs.cornell.edu/home/cardie/papers/ozan-nips14drsv.pdf">Deep Recursive Neural Networks for Compositionality in Language</a>, NIPS 2014</li>
       <li>Belanger and McCallum, <a href="https://arxiv.org/pdf/1511.06350.pdf">Structured Prediction Energy Networks</a>, ICML 2016</li>
   </ul>
<!--
   <li>

   <ul>
       <li> <a href="
       <li> <a href="
       <li> <a href="
   </ul>
-->

</ul>
</p> 

<a name="papers2"></a>
<h3> Related Courses, Tutorials and Textbooks</h3>

<br>  
<p class='indented'>
<ul>
     <li> <a href="http://pgm.stanford.edu/">Probabilistic Graphical Models: Principles and Techniques</a> by Koller and Friedman</li>
     <li> <a href="http://pub.ist.ac.at/~chl/papers/nowozin-fnt2011.pdf">Structured Learning and Prediction in Computer Vision</a> by Nowozin and Lampert</li>
     <li> <a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference</a> by Wainwright and Jordan</li>
     <li> <a href="https://ermongroup.github.io/cs228-notes/">CS 228: Probabilistic Graphical Models, Winter 2016/2017</a> by Stefano Ermon</li>
</ul>
</p>

</body>

