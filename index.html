<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>CS/CNS/EE 159: Special Topics in Machine Learning: Structured Prediction</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<link href="default.css" rel="stylesheet" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="http://identity.caltech.edu/site_images/275-caltech-icon-orange-rgb.png" />

</head>
<body>

   <h1>(CS/CNS/EE 159) Special Topics in Machine Learning: Structured Prediction </h1>

   <br>
   2016/2017 Spring Term (<a href="http://www.yisongyue.com/courses/cs159/">previous year</a>)



<a name="desc"></a>
<h3>Course Description</h3>
<p class="indented">
This course will cover a mixture of the following topics:
<ul>
   <li> Structured Prediction
   <li> Graphical Models
   <li> Deep Structured Prediction
</ul>
</p>

<h3>Course Details</h3>
<p class="indented">
<ul>
<li>Lectures on Tu/Th at 1pm-2:30pm in Annenberg 105</li>
<li>This is a paper reading course, where we read and discuss research papers in class</li>
<li>Student participation is required, including presenting papers in class (20% of total grade)</li>
<li>Mini-quiz on papers for every lecture, given after lecture/discussion (10% of total grade)</li>
<li>Final project that explores some topic covered in class (70% of final grade)</li>
<li>Piazza Forum: <a href="https://piazza.com/class/im3kskze9kv56g">link</a></li>
</ul>



<h3>Instructor</h3>
<p class="indented">
<a href="http://www.yisongyue.com">Yisong Yue</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yyue@caltech.edu<br>
<a href="http://ttic.uchicago.edu/~taehwan">Taehwan Kim</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;taehwan@caltech.edu<br>
</p>

<h3>Teaching Assistants</h3>
<p class="indented">
<table class="indented">
   <tr>
      <td>Hoang Le&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td> hmle@caltech.edu</td>
   </tr>

</table>
</p>


<h3>Office Hours</h3>
<iframe src="https://www.google.com/calendar/embed?showTitle=0&amp;showDate=0&amp;showPrint=0&amp;showTabs=0&amp;showCalendars=0&amp;mode=WEEK&amp;height=350&amp;wkst=1&amp;bgcolor=%23FFFFFF&amp;src=q0k7dfe04aiibvru2rtg20rdh4%40group.calendar.google.com&amp;color=%238C500B&amp;ctz=America%2FLos_Angeles" style=" border-width:0 " width="800" height="350" frameborder="0" scrolling="no"></iframe>

<a name="textbook"></a>
<h3>Optional Textbooks</h3>
<p class="indented">
<li><a href="http://www.cs.ubc.ca/~murphyk/MLbook/">Machine Learning: a Probabilistic Perspective</a>,
by <a href="http://research.google.com/pubs/KevinMurphy.html">Kevin Murphy</a> </li>
<li><a href="http://www.nowpublishers.com/article/Details/MAL-050">Convex Optimization: Algorithms and Complexity</a> (<a href="http://arxiv.org/pdf/1405.4980.pdf">Free Version</a>), by <a href="http://research.microsoft.com/en-us/um/people/sebubeck/">Sebastien Bubeck</a></li>
<li><a href="http://ciml.info/">A Course in Machine Learning</a>, by <a href="http://www.umiacs.umd.edu/~hal/">Hal Daume III</a></li>
Since this is an advanced level course, all relevant course materials can be learned via research papers and supplementary lecture notes.  However, these books are excellent references and I will refer to various chapters throughout the course.
</p>


<a name="lectures"></a>
<h3> Lectures & Recitation Schedule</h3>

<p class="indented"> Note: schedule is subject to change.<br><br>
</p>


<table class="indented schedule">
   <tr>
      <td class='schedule'></td>
      <td class='schedule'></td>
      <td class='schedule'></td>
      <td class='schedule'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td class='schedule'><b>Further Reading:</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
   </tr>
   <tr>
      <td class='schedule'>1/05/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Administrivia, Basics, Bias/Variance, Overfitting</td>
      <td class='schedule'>[<a href="lectures/Lecture_01.pdf">slides</a>]</td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>1/05/2017</td>
      <td class='schedule'>Recitation:</td>
      <td class='schedule'>Introduction to Python for Machine Learning</td>
      <td class='schedule'>[<a href="https://docs.google.com/presentation/d/17J67haaNtU258ZTUGwndria9m5-EBut4kbYGEaI-iJQ/edit?usp=sharing">slides</a>] </td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>1/10/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Perceptron, Gradient Descent</td>
      <td class='schedule'>[<a href="lectures/Lecture_02.pdf">slides</a>]</td>
      <td class='schedule'>Daume Chapter 3<br> Mistake Bounds for Perceptron [<a href="http://www.cs.nyu.edu/~mohri/pub/pmb.pdf">link</a>]<br>
         AdaGrad [<a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf">link</a>]<br>
         Stochastic Gradient Descent Tricks [<a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">link</a>]<br>
         Bubeck Chaper 3
      </td>
   </tr>
   <tr>
      <td class='schedule'>1/12/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>SVMs, Logistic Regression, Neural Nets, Loss Functions </td>
      <td class='schedule'>[<a href="lectures/Lecture_03.pdf">slides</a>]</td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>1/12/2017</td>
      <td class='schedule'>Recitation:</td>
      <td class='schedule'>Linear Algebra</td>
      <td class='schedule'>[<a href="lectures/recitation2.pdf">slides</a>][<a href="lectures/Recitation2Notebook.ipynb">iPython</a>]</td>
      <td class='schedule'> The Matrix Cookbook [<a href="http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274">link</a>]</td>
   </tr>
   <tr>
      <td class='schedule'>1/17/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Regularization, Lasso</td>
      <td class='schedule'>[<a href="lectures/Lecture_04.pdf">slides</a>]</td>
      <td class='schedule'>Murphy 13.3</td>
   </tr>
   <tr>
      <td class='schedule'>1/19/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Decision Trees, Bagging,  Random Forests</td>
      <td class='schedule'>[<a href="lectures/Lecture_05.pdf">slides</a>]</td>
      <td class='schedule'>Overview of Decision Trees [<a href="http://www.ise.bgu.ac.il/faculty/liorr/hbchap9.pdf">pdf</a>]<br>Overview of Bagging [<a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf">pdf</a>]<br>Overview of Random Forests [<a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">pdf</a>]</td>
   </tr>
   <tr>
      <td class='schedule'>1/19/2017</td>
      <td class='schedule'>Recitation:</td>
      <td class='schedule'>NO RECITATION</td>
      <td class='schedule'></td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>1/24/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Boosting, Ensemble Selection</td>
      <td class='schedule'>[<a href="lectures/Lecture_06.pdf">slides</a>]</td>
      <td class='schedule'>Shapire's Overview of Boosting [<a href="../lectures/msri.pdf">pdf</a>]</td>
   </tr>
   <tr>
      <td class='schedule'>1/26/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Deep Learning (taught by <a href="http://joelouismarino.github.io/">Joe Marino</a>)</td>
      <td class='schedule'>[<a href="lectures/Lecture_07.pdf">slides</a>]</td>
      <td class='schedule'>Deep Learning Book [<a href="http://www.deeplearningbook.org/">html</a>]<br>
A Brief Overview of Deep Learning. [<a href="http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html">link</a>]
      </td>
   </tr>
   <tr>
      <td class='schedule'>1/26/2017</td>
      <td class='schedule'>Recitation:</td>
      <td class='schedule'>Keras Tutorial</td>
      <td class='schedule'>[<a href="lectures/recitation3.pdf">slides</a>]</td>
      <td class='schedule'>[<a href="https://keras.io/">link</a>]</td>
   </tr>
   <tr>
      <td class='schedule'>1/31/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Deep Learning Part 2 (taught by <a href="http://joelouismarino.github.io">Joe Marino</a>)</td>
      <td class='schedule'>[<a href="lectures/Lecture_08.pdf">slides</a>]</td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>2/2/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Recent Applications</td>
      <td class='schedule'>[<a href="lectures/Lecture_09.pdf">slides</a>]</td>
      <td class='schedule'>
         Edge Detection [<a href="https://arxiv.org/abs/1406.5549">paper</a>]<br>
         Visual Speech [<a href="http://projects.yisongyue.com/visual_speech/">project</a>][<a href="http://www.yisongyue.com/publications/kdd2015_ssw_dt.pdf">paper</a>]</td>
   </tr>
   <tr>
      <td class='schedule'>2/2/2017</td>
      <td class='schedule'>Recitation:</td>
      <td class='schedule'>Probability &amp; Sampling</td>
      <td class='schedule'>[<a href="lectures/recitation4.pdf">slides</a>]</td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>2/7/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Probabilistic Models, Naive Bayes</td>
      <td class='schedule'>[<a href="lectures/Lecture_10.pdf">slides</a>]</td>
      <td class='schedule'>Murphy 3.5</td>
   </tr>
   <tr>
      <td class='schedule'>2/9/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Hidden Markov Models</td>
      <td class='schedule'>[<a href="lectures/Lecture_11.pdf">slides</a>][<a href="lectures/cs155-hmm-notes.pdf">notes</a>]</td>
      <td class='schedule'>Murphy 17.3--17.5</td>
   </tr>
   <tr>
      <td class='schedule'>2/9/2017</td>
      <td class='schedule'>Recitation:</td>
      <td class='schedule'>NO RECITATION</td>
      <td class='schedule'></td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>2/14/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'><b>**CANCELLED**</b> Deep Generative Models</td>
      <td class='schedule'></td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>2/14/2017 <br><b>TUESDAY <br>7-8pm</b></td>
      <td class='schedule'>Recitation:</td>
      <td class='schedule'>Dynamic Programming</td>
      <td class='schedule'>[<a href="lectures/recitation5.pdf">slides</a>]</td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>2/16/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Unsupervised Learning, Clustering, Dimensionality Reduction</td>
      <td class='schedule'>[<a href="lectures/Lecture_12.pdf">slides</a>]</td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>2/21/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Latent Factor Models, Non-Negative Matrix Factorization</td>
      <td class='schedule'>[<a href="lectures/Lecture_13.pdf">slides</a>]</td>
      <td class='schedule'>Original Netflix Paper [<a href="http://www.columbia.edu/~jwp2128/Teaching/W4721/papers/ieeecomputer.pdf">link</a>]</td>
   </tr>
   <tr>
      <td class='schedule'>2/23/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Embeddings</td>
      <td class='schedule'>[<a href="lectures/Lecture_14.pdf">slides</a>]</td>
      <td class='schedule'>Locally Linear Embedding [<a href="https://www.cs.nyu.edu/~roweis/lle/">link</a>]<br>Playlist Embedding [<a href="http://www.cs.cornell.edu/People/tj/playlists/">link</a>]<br>word2vec [<a href="https://code.google.com/archive/p/word2vec/">link</a>]</td>
   </tr>
   <tr>
      <td class='schedule'>2/23/2017</td>
      <td class='schedule'>Recitation:</td>
      <td class='schedule'>NO RECITATION</td>
      <td class='schedule'></td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>2/28/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Recent Applications</td>
      <td class='schedule'>[<a href="lectures/Lecture_15.pdf">slides</a>]</td>
      <td class='schedule'>Lasso for cancer detection [<a href="http://www.pnas.org/content/111/7/2436">paper</a>]<br>
         Badge dictionary learning from twitter [<a href="https://dl.dropboxusercontent.com/u/16830382/papers/badgepaper-kdd2013.pdf">paper</a>]<br>
         Deep learning for visual style [<a href="http://www.cs.cornell.edu/~andreas/iccv15.pdf">paper</a>] 
      </td>
   </tr>
   <tr>
      <td class='schedule'>3/2/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Deep Generative Models (taught by <a href="http://ttic.uchicago.edu/~taehwan/">Taehwan Kim</a>)</td>
      <td class='schedule'>[<a href="lectures/Lecture_16.pdf">slides</a>]</td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>3/2/2017</td>
      <td class='schedule'>Recitation:</td>
      <td class='schedule'>NO RECITATION</td>
      <td class='schedule'></td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>3/7/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Survey of Advanced Topics</td>
      <td class='schedule'>[<a href="lectures/Lecture_17.pdf">slides</a>]</td>
      <td class='schedule'></td>
   </tr>
   <tr>
      <td class='schedule'>3/9/2017</td>
      <td class='schedule'>Lecture:</td>
      <td class='schedule'>Review &amp; Q/A</td>
      <td class='schedule'></td>
      <td class='schedule'></td>
   </tr>
</table>

<h3>Additional References</h3>
<ul>
   <li>
   Stochastic Gradient Descent Tricks [<a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">link</a>]</li>
   <li>Papers on Ensemble Selection. [<a href="http://www.niculescu-mizil.org/papers/shotgun.icml04.revised.rev2.pdf">paper1</a>][<a href="http://www.niculescu-mizil.org/papers/enssel_most_long.pdf">paper2</a>][<a href="http://www.niculescu-mizil.org/papers/KDDCup09.pdf">KDD Cup Report</a>]</li>
   <li>Practical Bayesian Optimization for Efficient Grid Search of Tuning Parameters. [<a href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf">paper</a>][<a href="https://github.com/JasperSnoek/spearmint">software</a>]</li>
   <li>Reasonably Accessible Paper on Regularized Multi-Task Learning. [<a href="http://www0.cs.ucl.ac.uk/staff/M.Pontil/reading/mt-kdd.pdf">paper</a>]</li>
   <li>Overview of Topic Models. [<a href="http://www.cs.princeton.edu/~blei/papers/BleiLafferty2009.pdf">paper</a>]</li>
   <li>Overview of Structural SVMs. [<a href="http://www.yisongyue.com/publications/cacm2009_structsvm.pdf">paper</a>]</li>
   <li>A Brief Overview of Deep Learning. [<a href="http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html">link</a>]</li>
   <li>Tutorial on Learning Reductions. [<a href="http://hunch.net/~reductions_tutorial/">pdf</a>]</li>
   <li>The Matrix Cookbook (a lot of useful properties of matrices). [<a href="http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274">link</a>]</li>
   <li>Learning Reductions Overview. [<a href="http://arxiv.org/pdf/1502.02704v1.pdf">paper</a>]</li>
</ul>


</body>

