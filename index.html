<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>CS 159: Advanced Topics in Machine Learning: Structured Prediction</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<link href="default.css" rel="stylesheet" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="http://identity.caltech.edu/site_images/275-caltech-icon-orange-rgb.png" />

</head>
<body>

   <h1>CS 159: Advanced Topics in Machine Learning: Structured Prediction </h1>

   <br>
   2016/2017 Spring Term (<a href="http://www.yisongyue.com/courses/cs159/">previous year</a>)



<a name="desc"></a>
<h3>Course Description</h3>
<p class="indented">
This course will cover a mixture of the following topics:
<ul>
   <li> Graphical Models
   <li> Inference Methods
   <ul> 
        <li> Message Passing, Integer Programs, Dynamic Programming, Variational Methods
   </ul>
   <li> Classical Discriminative Learning
   <ul> 
        <li> Structured SVM, Structured Perceptron, Conditional Random Fields
   </ul>
   <li> Non-Linear Approaches
   <ul> 
        <li> Structured Random Forests, Deep Structured Prediction
   </ul>
   <li> More Complex Structures
   <ul>
        <li> Hierarchical Classification, Sequence Prediction/Generation
   </ul>
   <li> Applications to Computer Vision, Speech Recognition, Natural Language Processing, etc
</ul>
</p>

<h3>Course Details</h3>
<p class="indented">
<ul>
<li>Lectures on Tu/Th at 1pm-2:30pm in Annenberg 105</li>
<li>This is a paper reading course, where we read and discuss research papers in class</li>
<li>Student participation is required, including presenting papers in class (20% of total grade)</li>
<li>Mini-quiz on papers for every lecture, given after lecture/discussion (10% of total grade)</li>
<li>Final project that explores some topic covered in class (70% of final grade)</li>
<li>Piazza Forum: <a href="https://piazza.com/caltech/spring2017/cs159">link</a></li> 
</ul>



<h3>Instructor</h3>
<p class="indented">
<a href="http://ttic.uchicago.edu/~taehwan">Taehwan Kim</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;taehwan@caltech.edu<br>
<a href="http://www.yisongyue.com">Yisong Yue</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yyue@caltech.edu<br>
</p>

<h3>Teaching Assistants</h3>
<p class="indented">
<a href="http://hoangminhle.github.io/">Hoang Le</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hmle@caltech.edu<br>
Jialin Song</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jssong@caltech.edu<br>
<a href="http://stephanzheng.github.io/">Stephan Zheng</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stephan@caltech.edu<br>
</p>


<h3>Office Hours</h3>
<iframe src="https://calendar.google.com/calendar/embed?showTitle=0&amp;mode=WEEK&amp;height=600&amp;wkst=1&amp;bgcolor=%23FFFFFF&amp;src=dvolu8hvpm63i4kv98ga00md6s%40group.calendar.google.com&amp;color=%23B1365F&amp;ctz=America%2FLos_Angeles" style="border-width:0" width="800" height="600" frameborder="0" scrolling="no"></iframe>

<a name="lectures"></a>
<h3> Presentation Schedule</h3>

<p class="indented"> 
<br>Note: schedule is subject to change.<br><br>

<table class="indented">
   <tr>
      <td> <b>Date</b></td>
      <td> <b>Papers</b> </td>
      <td> <b>Presenters</b>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
      <td></td>
      <td> <b>Materials</b</td>
   </tr>
   <tr>
      <td> 4/4/2017&nbsp;</td>
      <td> Introduction &amp; Administrivia&nbsp;<br> Prababilistic Graphical Models&nbsp;<br> Inference Methods</td>
      <td> Taehwan Kim </td>
      <td>[<a href="lectures/lecture_04_04.pdf">slides</a>]</td>
      <td> <li><a href="http://www.cis.upenn.edu/~mkearns/papers/barbados/jordan-tut.pdf">An introduction to graphical models</a> <br>
         <li><a href="http://mlg.eng.cam.ac.uk/zoubin/course04/hbtnn2e-I.pdf">Probabilistic inference in graphical models</a> 
      </td>
   </tr>
</table>

<a name="papers"></a>
<h3>Topic and Reading List</h3>
<!--
<p class='indented'>
<b><a href="https://docs.google.com/spreadsheets/d/11dVEmrZ7rnvnSCxqIjN41bVQztDjpDV-RlDJZKHy7OM/edit#gid=0">Presentation Signup Sheet</a></b>
</p>
-->
<br>  
<p class='indented'>
<ul>
   <li> Inference Methods
   <ul>
        <li> Graph Cuts</li>
        <ul> 
             <li> Boykov et al, <a href="http://www.cs.sfu.ca/CourseCentral/821/li/material/source/Boykov01.pdf">Fast Approximate Energy Minimization via Graph Cuts, TPAMI 2001ine Learning</a>, TPAMI 2001.  </li>
             <li> Boykov et al, <a href="http://www.cs.duke.edu/brd/Teaching/Bio/asmb/current/Papers/BVZ-cvpr98.pdf">Markov Random Fields with Efficient Approximations</a>, CVPR 1998 </li>
        </ul>
        <li> Linear Programing </li>
        <ul>
             <li> Chapter 8.2 and 8.4 in <a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference</a> by Wainwright and Jordan</li>
             <li> Sontag et al, <a href="https://arxiv.org/abs/1206.3288">Tightening LP Relaxations for MAP using Message Passing</a>, UAI 2008</li>
        </ul>
        <li> Variational Inference</li>
        <ul>
             <li> Chapter 5 in <a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference</a> by Wainwright and Jordan</li>
             <li> <a href="https://arxiv.org/pdf/1601.00670.pdf">Variational Inference: A Review for Statisticians</a> by Blei</li>
        </ul>
   </ul>
   <li> Graphical Models</li>
   <ul>
        <li> Hidden Markov Models (HMM)</li>
        <ul>
             <li>Sha and Saul, <a href="https://papers.nips.cc/paper/3051-large-margin-hidden-markov-models-for-automatic-speech-recognition.pdf">Large margin hidden Markov models for automatic speech recognition</a>, NIPS 2007</li>
             <li>Altun et al, <a href="http://www.aaai.org/Papers/ICML/2003/ICML03-004.pdf">Hidden Markov Support Vector Machines</a>, ICML 2003</li>
             <li>Dahl et al, <a href="https://www.cs.toronto.edu/~gdahl/papers/DBN4LVCSR-TransASLP.pdf">Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition</a>, TASLP 2012</li>
        </ul>
        <li>Conditional Random Fields (CRF)</li>
        <ul>
             <li>Lafferty et al, <a href="https://faculty.cs.byu.edu/~ringger/CS479/papers/LaffertyMcCallumPereira-CRF-icml01.pdf">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</a>, ICML 2001</li>
             <li>Quattoni et al, <a href="https://papers.nips.cc/paper/2652-conditional-random-fields-for-object-recognition.pdf">Conditional Random Fields for Object Recognition</a>, NIPS 2004</li>
             <li> Zheng et al, <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf">Conditional Random Fields as Recurrent Neural Networks</a>, ICCV 2015</li>
        </ul>
   </ul> 
   <li>Topic Model</li>
   <ul>
        <li>Blei et al, <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Latent Dirichlet Allocation</a>, JMLR 2003</li>
        <li><a href="http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf">Topic models</a> by Blei and Lafferty</li>
   </ul>
   <li>Structured Support Vector Machines (SVM) </li>
   <ul>
       <li> Tsochantaridis et al, <a href="http://www.jmlr.org/papers/volume6/tsochantaridis05a/tsochantaridis05a.pdf">Large Margin Methods for Structured and Interdependent Output Variables</a>, JMLR 2005</li>
       <li> Yu and Joachims <a href="http://www.joachims.org/publications/yu_joachims_09a.pdf">Learning structural SVMs with latent variables</a>, ICML 2009</li>
       <li> Yue and Joachims <a href="http://www.yisongyue.com/publications/icml2008_svmdiv.pdf">Predicting Diverse Subsets Using Structural SVMs</a>, ICML 2008</li>
   </ul>
   <li> Hierarchical / Extreme Classification </li>
   <ul> 
       <li> Cai and Hofmann, <a href="https://info.cis.uab.edu/zhang/Spam-mining-papers/Hierarchical.Data.Classification.with.Support.Vector.Machines.pdf">Hierarchical document categorization with support vector machines</a>, CIKM 2004</li>
       <li> Yen et al, <a href="http://www.cs.utexas.edu/~inderjit/public_papers/pdsparse_icml16.pdf">A Primal and Dual Sparse Approach to Extreme Classification</a>, ICML 2016</li>
   </ul>
   <li> Structured Perceptron </li>
   <ul>
       <li> Collins, <a href="http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d4/data/pdf/anthology-PDF/W/W02/W02-1001.pdf">Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</a>, ACL 2002</li>
       <li> McDonald et al, <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36266.pdf">Distributed Training Strategies for the Structured Perceptron</a>, NAACL 2010</li>
   </ul>
   <li> Deep Structured Models</li>
   <ul>
       <li>MRF/CRF + Deep Learning</li>
       <ul>
            <li>Chen et al, <a href="https://arxiv.org/pdf/1407.2538">Learning Deep Structured Models</a>, ICML 2015</li>
            <li>Schwing and Urtasun, <a href="https://arxiv.org/pdf/1503.02351">Fully Connected Deep Structured Networks</a>, arxiv 2015</li>
            <li>Chen et al, <a href="https://arxiv.org/pdf/1412.7062">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a>, ICLR 2015</li>
       </ul>
       <li> Deep/Convolutional Neural Networks</li>
       <ul>
            <li>Sohn et al, <a href="https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf">Learning Structured Output Representation using Deep Conditional Generative Models</a>, NIPS 2015</li>
            <li>Dosovitskiy et al, <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf">Learning to Generate Chairs with Convolutional Neural Networks</a>, CVPR 2015</li>
            <li>Stewart and Ermon, <a href="https://arxiv.org/pdf/1609.05566">Label-Free Supervision of Neural Networks with Physics and Domain Knowledge</a>, arxiv 2016</li>
       </ul>
       <li> Recurrent Neural Networks</li>
       <ul>
            <li> Johnson et al, <a href="http://papers.nips.cc/paper/6379-composing-graphical-models-with-neural-networks-for-structured-representations-and-fast-inference.pdf">Composing graphical models with neural networks for structured representations and fast inference</a>, NIPS 2016</li>
            <li> Ranzatto et al, <a href="https://arxiv.org/pdf/1511.06732">Sequence Level Training with Recurrent Neural Networks</a>, arxiv 2015
            <li>Alvarez-Melis and Jaakkola, <a href="https://people.csail.mit.edu/tommi/papers/AlvJaa_ICLR2017.pdf">TREE-STRUCTURED DECODING WITH DOUBLYRECURRENT NEURAL NETWORKS</a>, ICLR 2017</li>
       </ul>
   </ul>
   <li> Sequence-to-sequence Model</li>
   <ul>
       <li> Kim et al, <a href="http://www.yisongyue.com/publications/kdd2015_ssw_dt.pdf"> A Decision Tree Framework for Spatiotemporal Sequence Prediction</a>, KDD 2015</li>
       <li> Sutskever et al, <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sequence to Sequence Learning with Neural Networks</a>, NIPS 2014</li>
   </ul>
   <li> Structured Random Forests </li>
   <ul>
       <li> Dollar and Zitnick, <a href="https://pdollar.github.io/files/papers/DollarICCV13edges.pdf">Structured Forests for Fast Edge Detection</a>, ICCV 2013</li>
       <li> Kontschieder et al, <a href="http://www.dsi.unive.it/~srotabul/files/publications/iccv11.pdf">Structured Class-Labels in Random Forests for Semantic Image Labelling</a>, ICCV 2011</li>
   </ul>
   <li> Image Captioning and Generation From Text</li>
   <ul>
       <li> Xu et al, <a href="https://arxiv.org/pdf/1502.03044.pdf">Attend and Tell: Neural Image Caption Generation with Visual Attention</a>, ICML 2015</li>
       <li> Mansimov et al, <a href="https://arxiv.org/pdf/1511.02793.pdf">Generating images from captions with attention</a>, ICLR 2016</li>
   </ul>

   <li> Optimization </li>
   <ul>
       <li> Weiss and Taskar, <a href="http://homes.cs.washington.edu/~taskar/pubs/aistats10cascades.pdf">Structured Prediction Cascades</a>, AISTATS 2010</li>
       <li> Shi et al, <a href="https://cs.stanford.edu/~pliang/papers/sample-aistats2015.pdf">Learning Where to Sample in Structured Prediction</a>, AISTATS 2015</li>
   </ul>

   <li> Active Learning</li>
   <ul>
       <li>Shivaswamy and Joachims, <a href="http://www.cs.cornell.edu/people/tj/publications/shivaswamy_joachims_12a.pdf">Online Structured Prediction via Coactive Learning</a>, ICML 2012</li>
       <li> Luo et al, <a href="http://papers.nips.cc/paper/4953-latent-structured-active-learning.pdf">Latent Structured Active Learning</a>, NIPS 2013</li>
   </ul>
<!--
   <li>

   <ul>
       <li> <a href="
       <li> <a href="
       <li> <a href="
   </ul>
-->
 
</ul>
</p>        



</body>

